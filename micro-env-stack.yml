version: '3.8'
#Netork Initialization
networks:
  micro-nework-frontend:
    driver: overlay
  micro-nework-backend:
    driver: overlay
  elastic-stack-network:
    driver: overlay


# Be sure that you have configure the remote volumes with NFS
# https://sysadmins.co.za/docker-swarm-persistent-storage-with-nfs/
volumes:
  # Elsatic search volume, this volume will be shared among the registered & Configured managers.
  session.db.vol:
    driver: nfs
    driver_opts:
      # Here you should replace this ip with the NFS server IP, the directory /mysql exists on the root folder of NFS shared + /mysql
      share: swarmNfs.server.io:/mysql/session
  # Elsatic search volume, this volume will be shared among the registered & Configured managers.
  elastic-volume.vol:
    driver: nfs
    driver_opts:
      # Here you should replace this ip with the NFS server IP, the directory /mysql exists on the root folder of NFS shared + /mysql
      share: swarmNfs.server.io:/elastic-volume

  # mysql.init.vol:
  #   driver: nfs
  #   driver_opts:
  #     # Here you should replace this ip with the NFS server IP, the directory /mysql exists on the root folder of NFS shared + /mysql
  #     share: nfs.server.io:/mysql-config
  # This volume must be created externally in order to configure the data persists correctely.
  # By mounting this volume directely on the local filesystem, no matter which container run will have the information of the previus beats running.
  # COnfigure each worker to have a volume like this in order to gather logs per worker Machine 
  # Run to create the volume :
  # docker volume create --driver local \
  #   --opt type=none \
  #   --opt device=/volumes/beats \
  #   --opt o=bind filebeat.prod.ext.vol
  filebeat.prod.ext.vol: 
    external: true
    name: filebeat.prod.ext.vol
  


services:
  session:
    # container_name: generalRDBMS
    image: private.registry.io/session-db:production
    volumes:
     - session.db.vol:/var/lib/mysql:rw
    #  - mysql.init.vol:/filesystem-schema.sql:/docker-entrypoint-initdb.d/schema.sql:ro
    networks:
      - micro-nework-backend
    deploy:
        placement:
            constraints:
              - "node.role==manager"
      #Number of replicas
        replicas: 1
      # Configure stack update
        restart_policy:
          condition: on-failure
    environment:
      MYSQL_ROOT_PASSWORD: root
      # MYSQL_DATABASE: sessionDB
      MYSQL_USER: root
      MYSQL_PASSWORD: root
  
    # Message Brokers
  
  # Rabbit MQ Server 
  rabbit-server:
    image: private.registry.io/rabbitmq:production
    hostname: "rabbit-server"
    environment:
      RABBITMQ_DEFAULT_USER: "abstract"
      RABBITMQ_DEFAULT_PASS: "admin"
      RABBITMQ_DEFAULT_VHOST: "/"
    ports:
      - "15672:15672" 
      - "5672:5672" 
      - "15671:15671" 
      - "5671:5671" 
      - "4369:4369" 
    networks:
      - micro-nework-frontend
    depends_on:
      - swarm-visualize
    deploy:
      placement:
        constraints:
          - "node.role==manager"
      replicas: 1
      restart_policy:
        condition: on-failure

  # Cloud COnfig server
  config-server:
    image: private.registry.io/config-server:production
    ports:
      - "8888:8888"
    healthcheck:
        test: ["CMD", "curl", "-f", "http://localhost:8888/actuator"]
        interval: 30s
        timeout: 10s
        retries: 5
    labels:
      collect_logs_with_filebeat: "false"
      decode_log_event_to_json_object: "false"
    networks:
      - micro-nework-frontend
    depends_on:
      - rabbit-server
      - swarm-visualize
    deploy:
      placement:
        constraints:
          - "node.role==manager"
      replicas: 1
      restart_policy:
        condition: on-failure

  #Centralized Logging ELastic tack
  elasticsearch:
    image: private.registry.io/elasticsearch:production
    user: root
    networks:
      - elastic-stack-network
    ports:
      - "9200:9200"
    environment:
      - "discovery.type=single-node"
    volumes:
      - elastic-volume.vol:/usr/share/elasticsearch/data              # Persistence data
    depends_on:
      - swarm-visualize
    deploy:
      placement:
        constraints:
          - "node.role==manager"
      replicas: 1
      restart_policy:
        condition: on-failure
    
  logstash:
    image: private.registry.io/logstash:production
    networks:
      - elastic-stack-network
    ports:
      - "25826:25826"
      - "5044:5044"
    volumes:
      - /mnt/local-nfs/logstash-conf/pipeline:/usr/share/logstash/pipeline:ro                # Pipeline configuration
    depends_on:
      - elasticsearch
    deploy:
      placement:
        constraints:
          - "node.role==manager"
      replicas: 1
      restart_policy:
        condition: on-failure

  kibana:
    image: private.registry.io/kibana:production
    networks:
      - elastic-stack-network
    ports:
      - "5601:5601"
    depends_on:
      - elasticsearch
    deploy:
      placement:
        constraints:
            -  "node.role==manager"
            #- "node.labels.proxy==true"
      replicas: 1
      restart_policy:
        condition: on-failure

  # Beats will be installed on the workers and it will scale as one container per worker
  # due to that we need to scale it externalyy depending on the number of workers we have (docker service update --replicas-max-per-node=1 local_filebeat)
  filebeat:
    image: private.registry.io/beats:production
    networks:
      - elastic-stack-network
    volumes:
      # For the beats configuration volume we need to find a way to share the config file git, nfs etc.
      # here we mount configuration files from the nfs to this directory
      - /nfs/micor-env/config/filebeat/filebeat.docker.yml:/usr/share/filebeat/filebeat.yml:ro # Configuration file
      - /var/lib/docker/containers:/var/lib/docker/containers:ro           # Docker logs
      - /var/run/docker.sock:/var/run/docker.sock:ro                       # Additional information about containers
      - filebeat.prod.ext.vol:/usr/share/filebeat/data:rw                        # Persistence data
    user: root                                                             # Allow access to log files and docker.sock
    depends_on:
      - logstash
    deploy:
      placement:
        max_replicas_per_node: 1
        constraints: 
          - node.role != manager
        #   - "node.labels.proxy!=true"
      replicas: 2
      restart_policy:
        condition: on-failure
  # Swarm Visualizer
  swarm-visualize:
    image: private.registry.io/visualizer:latest
    ports:
      - "8090:8080"
    networks:
      - micro-nework-frontend
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    deploy:
      placement:
        constraints:
          - "node.role==manager"
        #Number of replicas
      replicas: 1
      # Configure stack update
      restart_policy:
        condition: on-failure 

  # Discovery service 
  discovery-service:
    image: private.registry.io/discovery-service:production
    networks:
      - micro-nework-frontend
    ports:
        - "80:8761"
    healthcheck:
        test: ["CMD", "curl", "-f", "http://abstract:admin@localhost:8761/actuator"]
        interval: 30s
        timeout: 10s
        retries: 5 
    labels:
        collect_logs_with_filebeat: "false"
        decode_log_event_to_json_object: "false"
    depends_on:
      - config-server
    deploy:
      placement:
        constraints:
            -  "node.role==manager"
        #   - "node.labels.proxy==true"
      replicas: 1
      restart_policy:
        condition: on-failure

  zuul-proxy:
    image: private.registry.io/zuul-proxy:production
    #internal-links
    networks:
      - micro-nework-frontend
    links:
        - "discovery-service:discovery-service"  
    labels:
        collect_logs_with_filebeat: "true"
        decode_log_event_to_json_object: "true"
    depends_on:
      - config-server
      - discovery-service
      - session-service
    ports:
      - "8080:8080"
    deploy:
      placement:
        constraints:
          - "node.role!=manager"
        #   - "node.labels.proxy!=true"
      replicas: 1
      restart_policy:
        condition: on-failure
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/actuator"]
      interval: 30s
      timeout: 10s
      retries: 5     
  
  session-service:
    image: private.registry.io/session-service:production
    networks:
      - micro-nework-frontend
      - micro-nework-backend
    links:
        - "discovery-service:discovery-service"  
    labels:
        collect_logs_with_filebeat: "true"
        decode_log_event_to_json_object: "true"
    depends_on:
      - config-server
      - discovery-service
      - graph-service
    deploy:
      placement:
        constraints:
          - "node.role!=manager"
        #   - "node.labels.proxy!=true"
      replicas: 1
      restart_policy:
        condition: on-failure
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/actuator"]
      interval: 30s
      timeout: 10s
      retries: 5

  graph-service:
    image: private.registry.io/graph-api:production
    #internal-links
    networks:
      - micro-nework-frontend
    links:
        - "discovery-service:discovery-service"  
    labels:
        collect_logs_with_filebeat: "true"
        decode_log_event_to_json_object: "true"
    depends_on:
      - config-server
      - discovery-service
    deploy:
      placement:
        constraints:
          - "node.role!=manager"
        #   - "node.labels.proxy!=true"
      replicas: 1
      restart_policy:
        condition: on-failure
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8999/actuator"]
      interval: 30s
      timeout: 10s
      retries: 5

  #Hello Service
  hello-service:
    image: private.registry.io/hello-service
    # ports:
    #     - "8080:8080"
    #internal-links
    networks:
      - micro-nework-frontend
    links:
        - "discovery-service:discovery-service"  
    labels:
        collect_logs_with_filebeat: "true"
        decode_log_event_to_json_object: "true"
    depends_on:
      - config-server
      - discovery-service
    deploy:
      placement:
        constraints:
          - "node.role!=manager"
      replicas: 1
      restart_policy:
        condition: on-failure
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/actuator"]
      interval: 30s
      timeout: 10s
      retries: 5 
    # deploy:
    #   replicas: 2

  #hello Client
  hello-client:
    image: private.registry.io/hello-client:latest
    networks:
      - micro-nework-frontend
    links:
        - "discovery-service:discovery-service"
        - "config-server:config-server"  
    labels:
        collect_logs_with_filebeat: "true"
        decode_log_event_to_json_object: "true"
    depends_on:
      - config-server
      - discovery-service
    deploy:
      placement:
        constraints:
          - "node.role!=manager"
      replicas: 1
      restart_policy:
        condition: on-failure
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/actuator"]
      interval: 30s
      timeout: 10s
      retries: 5

  python-service:
    image: private.registry.io/python-service:production
    ports:
        - "5000:5000"
    networks:
      - micro-nework-frontend
    links:
        - "discovery-service:discovery-service"
        - "config-server:config-server"  
    labels:
        collect_logs_with_filebeat: "true"
        decode_log_event_to_json_object: "true"
    deploy:
        placement:
          constraints:
            - "node.role!=manager"
        replicas: 1
        restart_policy:
            condition: on-failure
    depends_on:
        - rabbit-server
        - config-server
        - discovery-service
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/service-status"]
      interval: 30s
      timeout: 10s
      retries: 5

  celery-worker:
    image: private.registry.io/celery-worker:production
    networks:
      - micro-nework-frontend
    links:
        - "discovery-service:discovery-service"
        - "config-server:config-server"  
    labels:
        collect_logs_with_filebeat: "true"
        decode_log_event_to_json_object: "true"
    deploy:
        placement:
          constraints:
            - "node.role!=manager"
        replicas: 1
        restart_policy:
            condition: on-failure
    depends_on:
        - rabbit-server
        - config-server
        - discovery-service
        - python-service
