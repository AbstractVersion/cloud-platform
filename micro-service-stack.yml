version: '3.7'
#Netork Initialization
networks:
  micro-nework-frontend:
    driver: overlay
  micro-nework-backend:
    driver: overlay
  elastic-stack-network:
    driver: overlay
  kafka-broker-network:
    driver: overlay

# Be sure that you have configure the remote volumes with NFS
# https://sysadmins.co.za/docker-swarm-persistent-storage-with-nfs/
volumes:
  # Elsatic search volume, this volume will be shared among the registered & Configured managers.
  mysql.vol:
    driver: nfs
    driver_opts:
      # Here you should replace this ip with the NFS server IP, the directory /mysql exists on the root folder of NFS shared + /mysql
      share: nfs.server.io:/mysql
  # Elsatic search volume, this volume will be shared among the registered & Configured managers.
  elastic-volume.vol:
    driver: nfs
    driver_opts:
      # Here you should replace this ip with the NFS server IP, the directory /mysql exists on the root folder of NFS shared + /mysql
      share: nfs.server.io:/elastic-volume
  
  # This volume must be created externally in order to configure the data persists correctely.
  # By mounting this volume directely on the local filesystem, no matter which container run will have the information of the previus beats running.
  # COnfigure each worker to have a volume like this in order to gather logs per worker Machine 
  # Run to create the volume :
  # docker volume create --driver local \
  #   --opt type=none \
  #   --opt device=/volumes/beats \
  #   --opt o=bind filebeat.ext.vol
  filebeat.ext.vol: 
    external: true
    name: filebeat.ext.vol
  # Shared volume by NFS containing filebeat configuration yml.
  # filebeat.ext.conf.vol: 
    # driver: nfs
    # driver_opts:
    #   # Here you should replace this ip with the NFS server IP, the directory /mysql exists on the root folder of NFS shared + /mysql
    #   share: 192.168.2.4:/filebeat-conf
    # driver: local
    # driver_opts:
    #   type: 'none'
    #   o: 'bind'
    #   device: '/home/ghost/test/filebeat-conf/'

      # /home/worker
  
      

services:

  swarm-visualize:
    image: private.registry.io/visualizer:latest
    ports:
      - "8090:8080"
    networks:
      - micro-nework-frontend
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    deploy:
      placement:
        constraints:
          - "node.role==manager"
        #Number of replicas
      replicas: 1
      # Configure stack update
      restart_policy:
        condition: on-failure  

  zookeeper:
    image: private.registry.io/zookeeper:latest
    networks:
      - kafka-broker-network
    expose:
      - "2181"
    deploy:
      placement:
        constraints:
          - "node.role==manager"
      replicas: 1
      restart_policy:
        condition: on-failure    
  
  generalRDBMS:
    # container_name: generalRDBMS
    image: private.registry.io/mariadb:latest
    volumes:
     - mysql.vol:/var/lib/mysql
    networks:
      - micro-nework-backend
    depends_on:
      - swarm-visualize
    # restart: on-failure 
    deploy:
        placement:
            constraints:
              - "node.role==manager"
      #Number of replicas
        replicas: 1
      # Configure stack update
        restart_policy:
          condition: on-failure
    environment:
      MYSQL_ROOT_PASSWORD: root
      MYSQL_DATABASE: sessionDB
      MYSQL_USER: root
      MYSQL_PASSWORD: root
    # Check for mysql connection
    # healthcheck:
    #   test: ["CMD", "curl", "-f", "http://localhost"]
    #   interval: 1m #1m30s
    #   timeout: 10s
    #   retries: 3
    #   start_period: 40s
  # Message Brokers
  
  # Rabbit MQ Server 
  rabbit-server:
    image: private.registry.io/rabbitmq:3-management
    hostname: "rabbit-server"
    environment:
      RABBITMQ_DEFAULT_USER: "abstract"
      RABBITMQ_DEFAULT_PASS: "admin"
      RABBITMQ_DEFAULT_VHOST: "/"
    ports:
      - "15672:15672" 
      - "5672:5672" 
      - "15671:15671" 
      - "5671:5671" 
      - "4369:4369" 
    networks:
      - micro-nework-frontend
    depends_on:
      - swarm-visualize
    deploy:
      placement:
        constraints:
          - "node.role==manager"
      replicas: 1
      restart_policy:
        condition: on-failure

  kafka:
    image: private.registry.io/kafka:latest
    deploy:
      replicas: 3
      restart_policy:
        condition: on-failure
    ports:
      - target: 9094
        published: 9094
        protocol: tcp
        mode: host
    environment:
      HOSTNAME_COMMAND: "docker info | grep ^Name: | cut -d' ' -f 2"
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: INSIDE://:9092,OUTSIDE://_{HOSTNAME_COMMAND}:9094
      KAFKA_LISTENERS: INSIDE://:9092,OUTSIDE://:9094
      KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      - zookeeper
    healthcheck:
      test:
        ["CMD", "kafka-topics.sh", "--list", "--zookeeper", "zookeeper:2181"]
      interval: 30s
      timeout: 10s
      retries: 4
  # Cloud COnfig server
  config-server:
    image: private.registry.io/config-server:latest
    ports:
      - "8888:8888"
    healthcheck:
        test: ["CMD", "curl", "-f", "http://localhost:8888/actuator"]
        interval: 30s
        timeout: 10s
        retries: 5
    labels:
      collect_logs_with_filebeat: "false"
      decode_log_event_to_json_object: "false"
    networks:
      - micro-nework-frontend
    depends_on:
      - rabbit-server
      - swarm-visualize
    deploy:
      placement:
        constraints:
          - "node.role==manager"
      replicas: 1
      restart_policy:
        condition: on-failure

  #Centralized Logging ELastic tack
  elasticsearch:
    image: private.registry.io/elasticsearch:latest
    user: root
    networks:
      - elastic-stack-network
    ports:
      - "9200:9200"
    environment:
      - "discovery.type=single-node"
    volumes:
      - elastic-volume.vol:/usr/share/elasticsearch/data              # Persistence data
    depends_on:
      - swarm-visualize
    deploy:
      placement:
        constraints:
          - "node.role==manager"
      replicas: 1
      restart_policy:
        condition: on-failure
    
  logstash:
    image: private.registry.io/logstash:latest
    networks:
      - elastic-stack-network
    ports:
      - "25826:25826"
      - "5044:5044"
    volumes:
      - /nfs/micor-env/config/logstash/pipeline:/usr/share/logstash/pipeline:ro                # Pipeline configuration
    depends_on:
      - elasticsearch
    deploy:
      placement:
        constraints:
          - "node.role==manager"
      replicas: 1
      restart_policy:
        condition: on-failure

  kibana:
    image: private.registry.io/kibana:latest
    networks:
      - elastic-stack-network
    ports:
      - "5601:5601"
    depends_on:
      - elasticsearch
    deploy:
      placement:
        constraints:
          - "node.role==manager"
      replicas: 1
      restart_policy:
        condition: on-failure

  # Beats will be installed on the workers and it will scale as one container per worker
  # due to that we need to scale it externalyy depending on the number of workers we have (docker service update --replicas-max-per-node=1 local_filebeat)
  filebeat:
    image: private.registry.io/beats:latest
    networks:
      - elastic-stack-network
    volumes:
      # For the beats configuration volume we need to find a way to share the config file git, nfs etc.
      # here we mount configuration files from the nfs to this directory
      - /nfs/micor-env/config/filebeat/filebeat.docker.yml:/usr/share/filebeat/filebeat.yml:ro # Configuration file
      - /var/lib/docker/containers:/var/lib/docker/containers:ro           # Docker logs
      - /var/run/docker.sock:/var/run/docker.sock:ro                       # Additional information about containers
      - filebeat.ext.vol:/usr/share/filebeat/data:rw                        # Persistence data
    user: root                                                             # Allow access to log files and docker.sock
    depends_on:
      - logstash
    deploy:
      placement:
        constraints: 
          - node.role != manager
      replicas: 1
      restart_policy:
        condition: on-failure

  # Discovery service 
  discovery-service:
    image: private.registry.io/discovery-service
    networks:
      - micro-nework-frontend
    ports:
        - "80:8761"
    healthcheck:
        test: ["CMD", "curl", "-f", "http://abstract:admin@localhost:8761/actuator"]
        interval: 30s
        timeout: 10s
        retries: 5 
    labels:
        collect_logs_with_filebeat: "false"
        decode_log_event_to_json_object: "false"
    depends_on:
      - config-server
    deploy:
      placement:
        constraints:
          - "node.role==manager"
      replicas: 1
      restart_policy:
        condition: on-failure

  #Hello Service
  hello-service:
    image: private.registry.io/hello-service
    # ports:
    #     - "8080:8080"
    #internal-links
    networks:
      - micro-nework-frontend
    links:
        - "discovery-service:discovery-service"  
    labels:
        collect_logs_with_filebeat: "true"
        decode_log_event_to_json_object: "true"
    depends_on:
      - config-server
      - discovery-service
    deploy:
      placement:
        constraints:
          - "node.role!=manager"
      replicas: 1
      restart_policy:
        condition: on-failure
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/actuator"]
      interval: 30s
      timeout: 10s
      retries: 5 
    # deploy:
    #   replicas: 2

  zuul-proxy:
    image: private.registry.io/zuul-proxy:latest
    #internal-links
    networks:
      - micro-nework-frontend
    links:
        - "discovery-service:discovery-service"  
    labels:
        collect_logs_with_filebeat: "true"
        decode_log_event_to_json_object: "true"
    depends_on:
      - config-server
      - discovery-service
      - session-service
    ports:
      - "8080:8080"
    deploy:
      placement:
        constraints:
          - "node.role!=manager"
      replicas: 1
      restart_policy:
        condition: on-failure
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/actuator"]
      interval: 30s
      timeout: 10s
      retries: 5     
  
  session-service:
    image: private.registry.io/session-service:latest
    networks:
      - micro-nework-frontend
      - micro-nework-backend
    links:
        - "discovery-service:discovery-service"  
    labels:
        collect_logs_with_filebeat: "true"
        decode_log_event_to_json_object: "true"
    depends_on:
      - config-server
      - discovery-service
      - graph-service
    deploy:
      placement:
        constraints:
          - "node.role!=manager"
      replicas: 1
      restart_policy:
        condition: on-failure
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/actuator"]
      interval: 30s
      timeout: 10s
      retries: 5

  graph-service:
    image: private.registry.io/graph-api
    #internal-links
    networks:
      - micro-nework-frontend
    links:
        - "discovery-service:discovery-service"  
    labels:
        collect_logs_with_filebeat: "true"
        decode_log_event_to_json_object: "true"
    depends_on:
      - config-server
      - discovery-service
    deploy:
      placement:
        constraints:
          - "node.role!=manager"
      replicas: 1
      restart_policy:
        condition: on-failure
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8999/actuator"]
      interval: 30s
      timeout: 10s
      retries: 5

  #Python Service
  python-service:
    build:
      context: ./python-service-model
      dockerfile: Dockerfile
    image: private.registry.io/python-api:latest
    depends_on:
      - config-server
      - discovery-service
    networks:
      - micro-nework-frontend
    #internal-links
    links:
        - "discovery-service:discovery-service"  
    labels:
        collect_logs_with_filebeat: "true"
        decode_log_event_to_json_object: "true"
    deploy:
      placement:
        constraints:
          - "node.role!=manager"
      replicas: 1
      restart_policy:
        condition: on-failure
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5001/api/info"]
      interval: 30s
      timeout: 10s
      retries: 5

  #hello Client
  hello-client:
    image: private.registry.io/hello-client:latest
    networks:
      - micro-nework-frontend
    links:
        - "discovery-service:discovery-service"
        - "config-server:config-server"  
    labels:
        collect_logs_with_filebeat: "true"
        decode_log_event_to_json_object: "true"
    depends_on:
      - config-server
      - discovery-service
    deploy:
      placement:
        constraints:
          - "node.role!=manager"
      replicas: 1
      restart_policy:
        condition: on-failure
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/actuator"]
      interval: 30s
      timeout: 10s
      retries: 5


  #Kafka Services
  producer-service:
    image: private.registry.io/library-producer:latest
    ports:
      - "8089:8080"
    depends_on:
      - zookeeper
      - kafka
      # - mongodb
    deploy:
      placement:
        constraints:
          - "node.role!=manager"
      replicas: 1
      # restart_policy:
      #   condition: on-failure
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/actuator"]
      interval: 30s
      timeout: 10s
      retries: 5 